% !TEX root = ../preamble.tex

\section{Finding the most frequently occurring genres}
In the following section, the problem is discussed, as well as different approaches to solving it.\marginpar{what problem? the problem discussed in ...}

\subsection{Finding heavy hitters}
Different algorithms solves the problem of finding the heavy hitters \textit{H} in a data set \textit{S}. A naïve approach is to store all distinct roles in the data set, and their respective count. A space concerned approach to finding the heavy hitters, as described in the section describing the Misra Gries algorithm, can reduce the space consumption.\marginpar{Reservoir sampling as well}

\subsection{Naïve solution}
The naïve solution stores all distinct roles in order to find the roles with a frequency above the threshold. The algorithm contains a map with every distinct role as key, and their respective count as the corresponding value. The time complexity is O(m) while the worst case space consumptions is O(m).\marginpar{Ninh was /care with time complexity}

\subsection{Reservoir Sampling}
Reservoir sampling is an algorithm that chooses \(k\) random elements from a dataset. It has a reservoir, \(Res\), of size \(k\) and a set of genres returned by the algorithm, \(R\). It holds that \(R \in Res\). Reservoir sampling can report some genres as being a heavy hitter even though it is not, i.e. the following might hold \(\exists x \left(x \in R \land x \notin H \right)\). Futhermore some heavy hitters are not included in the set of elements returned by the algorithm, i.e the following might hold \(\exists x \left(x \in H \land x \notin R \right)\).\\

Each item, \(a_i\), in \(S\) must have an equal chance of ending up in \(Res\). This is achievable by putting items \(a_1 \dots a_k\) in the reservoir. Every item after \(a_k\) has chance equal to \(\frac{1}{i}\) to be inserted in the reservoir at a random location. By doing this every item has a chance of \(\frac{1}{m}\) to be in \(Res\).

Equation~\ref{eq:one_over_m} proofs that some \(a_\ell\), \(1 \le \ell \le m\), has a chance of \(\frac{1}{m}\) to be in \(Res\), if the size of the reservoir is exactly one.
\begin{equation}
	\label{eq:one_over_m}
	\begin{split}
	Pr[a_\ell \textrm{ ends in } Res]\ 
	& = Pr[a_\ell \textrm{ is put in } Res\ \cdot \prod_{i=\ell+1}^{m} Pr[a_i \textrm{ does not replace } a_\ell] \\
	& = \frac{1}{\ell} \cdot \prod_{i=\ell+1}^{m}\left(1-\frac{1}{i}\right) \\
	& = \frac{1}{\ell} \cdot \prod_{i=\ell+1}^{m}\frac{i-1}{i} \\
	& = \frac{1}{m}
	\end{split}
\end{equation}

Generally a reservoir size of \(k\), \(k > 1\), is desirable in order to get a more precise estimate of the frequency of items in the stream. Since a proof showing that every subset of \(S\) with size \(k\) has an equal chance of being picked is along the same lines as a proof showing that every subset of \(S\) with size 2, the latter will be proofed for the sake om simplicity.
\begin{equation}
	\label{eq:one_over_m2}
	\begin{split}
	Pr[a_{\ell_1}, a_{\ell_2} \textrm{ ends in } Res]\ 
	& = \frac{2}{\ell_2} \cdot \prod_{\ell_1<i<\ell_2} \left(1- \frac{2}{i} + \frac{2}{i} \cdot \frac{1}{2}\right) \cdot \frac{2}{\ell_2} \cdot \frac{1}{2} \cdot \prod_{\ell_2 < j \le m} \left( 1 - \frac{2}{j} \right)  \\
	& = \frac{2}{\ell_1 \cdot \ell_2} \cdot \prod_{\ell_1 < i < \ell_2} \frac{i-1}{i} \cdot \prod_{\ell_2 < j \le m} \frac{j-2}{j} \\
	& = \frac{2}{\left( m - 1 \right) \cdot m} \\
	& = 1\Big/\tbinom{m}{2}
	\end{split}
\end{equation}

As seen in equation~\ref{eq:one_over_m2} every subset of \(S\) with size two has an equal chance of ending up in \(Res\). It follows that every subset of \(S\) with size \(k\) has an equal chance of being the final \(Res\).

For the algorithm to be classified as a streaming algorithm, its space consumption must be sublinear in the length of the stream. It achieves this by having the size of the reservoir fixed. To find the optimal size of the reservoir two variables are considered, \(\epsilon\) and \(\delta\). \(\epsilon\) is the error margin that one wants to achieve with probability equal to \(1 - \delta\).

The algorithm is relevant due to its time complexity and space consumption. The time complexity is \(O(n)\), and the space consumption is \(O(\frac{t}{\alpha})\).

As with the naïve solution, the algorithm only require a single pass through initial the dataset. After the first pass through, the algorithm counts the frequency of each item in the reservoir and reports the element if the frequency is equal to or exceeds the given threshold.
Furthermore, it requires considerably less space than the naïve solution at the expense of guaranteeing that the result is correct. It does so by having a single reservoir array with size \(O(\frac{t}{\alpha})\), defined as mentioned above. At first the array is filled with the first \(O(\frac{t}{\alpha})\) elements in the dataset, hereafter all new elements have a chance, equal to \(\frac{t/\alpha}{n'}\) where \textit{n'} is the number of elements that have been processed at the given point in time,
 of being swapped with a random element already in the the array.

%The threshold \textit{t} is the minimum amount of times a given role, must be present in the reservoir in order to be reported as included in \(H\).\ \\
% and some heavy hitters might not  \textit{R}, is in \textit{H}, such that \(R \subseteq H\), but not necessarily \(H \subseteq R\). Therefore the algorithm might report false negatives.




 
\subsection{Misra Gries}
The Misra Gries algorithm finds heavy hitters in a data stream. It maintains a map with up to \textit{k} entries, with keys being data element identifiers. If the set of element keys in the map is denoted \textit{K}, then \(H \subseteq K\), when the algorithm has finished. The size of the map is \(k = \frac{1}{\alpha} + 1\), meaning that the fraction an element has to constitute of \textit{S} to be in \textit{H} has an inverse relation to \textit{k}.
For each element in the data stream, the map is updated. If the key of the element already exists in the map, the value is incremented by 1. Else if the size of the map is less than \textit{k}, then the element is added to the map, with a value of 1. If the element does not exist in the map and the size of the map is \textit{k}, then the value of each key is decremented by 1 and keys with value zero is removed. This decrement procedure is repeated until the size of the map is less than \textit{k} such that the element can be added to the map.

\subsubsection{Worst case example}
The soundness of the algorithm can be explained by constructing a worst case scenario. If \(H \subseteq K\), then an element occurring \(\alpha\) times must be included in the final map. Consider a stream, with an element \textit{e} being in \textit{H} with \(\alpha = 0.2\). The stream consists of 10 elements. The map has 6 entries in this example. \textit{e} occurs twice as the first two, and the most effective way of reducing the count of \textit{e} is by filling the map and presenting an element that is not yet included in the map. Because it takes 5 distinct elements to fill the map and one additional element to reduce the count of all elements, 6 elements are required to reduce \textit{e}. The stream has 10 elements and therefore this can only happen once. Therefore the count of \textit{e} is at least 1, when the algorithm terminates.

\subsubsection{Additional filtering}
As Misra Gries returns false positives, additional filtering is needed for K=H. This is done by running the naïve solution, however the counting map only counts elements in K, which makes it as space efficient as Misra Gries.\marginpar{Den skal gerne rettes til at være single pass. False positives og false negatives er tilladte. Der skal beskrives sandsynligeheder for korrektheden}

\subsubsection{Space consumption}
The auxiliary space used by Misra Gries is \(O\left(\frac{1}{\alpha}\right)\), but as \(\alpha\) is a constant in many applications, this is regarded as constant space consumption.\marginpar{Hvornår er alpha ikke en konstant}

\subsubsection{String alignment}
The data source contains roles of different formats. To make roles that seem similar be regarded as the same string and remove uninteresting roles, the data is cleaned before being parsed to the algorithm. This involves removing strings tagged with symbols and strings that is not regarded as the kind of roles that we wish to investigate.
