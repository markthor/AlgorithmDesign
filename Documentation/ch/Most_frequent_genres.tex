% !TEX root = ../preamble.tex

\section{Finding the most frequently occurring roles}
In this section, the first problem presented in section~\ref{sub:problem} is discussed, as well as different approaches to solving it.

\subsection{Terminology and notation}
A role is a string that represents an actor’s role in a movie. The data stream, denoted \textit{S}, contains roles, with some of them being identical. The set of most occurring roles are called heavy hitters, denoted \textit{H} and are defined by \(\alpha\), the fraction of \textit{S}, that a role constitutes to be in \textit{H}.
Let the number of roles in \textit{S} be denoted \textit{m}, and the number of occurrences of a role in \textit{S} denoted \textit{c} then the role is in \textit{H} if and only if \(\alpha \le \frac{c}{m}\). %The threshold of the reservoir sampling algorithm is denoted \textit{t}.

\subsection{Finding heavy hitters}
Different algorithms solves the problem of finding the heavy hitters in a data set. A naïve approach is to store all distinct roles in the data set, and their respective count. A space concerned approach to finding the heavy hitters, for instance using Reservoir sampling, as described in section~\ref{sub:reservoir}, or the Misra Gries algorithm, as described in section~\ref{sub:misragries}, can reduce the space consumption.

\subsection{Naïve solution}
The naïve solution stores all distinct roles in order to find the heavy hitters. The algorithm contains a map with every distinct role as key, and their respective count as the corresponding value. The time complexity is O(m) while the worst case space consumptions is O(m).

\subsection{Reservoir Sampling}
\label{sub:reservoir}
Reservoir sampling is an algorithm that chooses \(k\) random elements from a dataset. It has a reservoir, \(Res\), of size \(k\) and a set of roles returned by the algorithm, \(R\), as begin heavy hitters. It holds that \(R \in Res\). Reservoir sampling can report some roles as being a heavy hitter even though it is not, i.e. the following might hold \(\exists x \left(x \in R \land x \notin H \right)\). Futhermore some heavy hitters are not included in the set of elements returned by the algorithm, i.e the following might hold \(\exists x \left(x \in H \land x \notin R \right)\).\\
Each item, \(a_i\), in \(S\) must have an equal chance of ending up in \(Res\). This is achievable by putting items \(a_1,\ \dots,\ a_k\) in the reservoir. Every item after \(a_k\) has chance equal to \(\frac{1}{i}\) to be inserted in the reservoir at a random location. By doing this every item has a chance of \(\frac{1}{m}\) to be in \(Res\).

Equation~\ref{eq:one_over_m} proofs that some \(a_\ell\), \(1 \le \ell \le m\), has a chance of \(\frac{1}{m}\) to be in \(Res\), if the size of the reservoir is exactly one.
\begin{equation}
	\label{eq:one_over_m}
	\begin{split}
	Pr[a_\ell \textrm{ ends in } Res]\ 
	& = Pr[a_\ell \textrm{ is put in } Res\ \cdot \prod_{i=\ell+1}^{m} Pr[a_i \textrm{ does not replace } a_\ell] \\
	& = \frac{1}{\ell} \cdot \prod_{i=\ell+1}^{m}\left(1-\frac{1}{i}\right) \\
	& = \frac{1}{\ell} \cdot \prod_{i=\ell+1}^{m}\frac{i-1}{i} \\
	& = \frac{1}{m}
	\end{split}
\end{equation}

Generally a reservoir size of \(k > 1\), is desirable in order to get a more precise estimate of the frequency of items in the stream. Since a proof showing that every subset of \(S\) with size \(k\) has an equal chance of being picked is along the same lines as a proof showing that every subset of \(S\) with size 2, the latter will be proven for the sake om simplicity.
\begin{equation}
	\label{eq:one_over_m2}
	\begin{split}
	Pr[a_{\ell_1}, a_{\ell_2} \textrm{ ends in } Res]\ 
	& = \frac{2}{\ell_2} \cdot \prod_{\ell_1<i<\ell_2} \left(1- \frac{2}{i} + \frac{2}{i} \cdot \frac{1}{2}\right) \cdot \frac{2}{\ell_2} \cdot \frac{1}{2} \cdot \prod_{\ell_2 < j \le m} \left( 1 - \frac{2}{j} \right)  \\
	& = \frac{2}{\ell_1 \cdot \ell_2} \cdot \prod_{\ell_1 < i < \ell_2} \frac{i-1}{i} \cdot \prod_{\ell_2 < j \le m} \frac{j-2}{j} \\
	& = \frac{2}{\left( m - 1 \right) \cdot m} \\
	& = 1\Big/\tbinom{m}{2}
	\end{split}
\end{equation}

As seen in equation~\ref{eq:one_over_m2} every subset of \(S\) with size two has an equal chance of ending up in \(Res\). It follows that every subset of \(S\) with size \(k\) has an equal chance of being the final \(Res\). \\

For the algorithm to be classified as a streaming algorithm, its space consumption must be sublinear in the length of the stream. It achieves this by having the size of the reservoir fixed. To find the optimal size of the reservoir two variables are considered, \(\epsilon\) and \(\delta\). \(\epsilon\) is the error margin that one wants to achieve with probability equal to \(1 - \delta\). Considering the algorithm estimates heavy hitters it is relevant to determine the probability that the estimate is within the error margin one allows. The size of the reservoir is dependent on these two variables. It can be calculated using the formula \(k=O\left(1/\epsilon^2\log\left(1/\delta\right)\right)\).\cite{reservoir}

%The algorithm is relevant due to its time complexity and space consumption. The time complexity is \(O(n)\), and the space consumption is \(O(\frac{t}{\alpha})\).
%As with the naïve solution, the algorithm only require a single pass through initial the dataset. After the first pass through, the algorithm counts the frequency of each item in the reservoir and reports the element if the frequency is equal to or exceeds the given threshold.
%Furthermore, it requires considerably less space than the naïve solution at the expense of guaranteeing that the result is correct. It does so by having a single reservoir array with size \(O(\frac{t}{\alpha})\), defined as mentioned above. At first the array is filled with the first \(O(\frac{t}{\alpha})\) elements in the dataset, hereafter all new elements have a chance, equal to \(\frac{t/\alpha}{n'}\) where \textit{n'} is the number of elements that have been processed at the given point in time,
% of being swapped with a random element already in the the array.
%The threshold \textit{t} is the minimum amount of times a given role, must be present in the reservoir in order to be reported as included in \(H\).\ \\
% and some heavy hitters might not  \textit{R}, is in \textit{H}, such that \(R \subseteq H\), but not necessarily \(H \subseteq R\). Therefore the algorithm might report false negatives.

\subsection{Misra Gries}
\label{sub:misragries}
The Misra Gries algorithm finds heavy hitters in a data stream. It maintains a map with up to \textit{k} entries, with keys being data element identifiers. If the set of element keys in the map is denoted \textit{K}, then \(H \subseteq K\), when the algorithm has finished. The size of the map is \(k = \frac{1}{\alpha} + 1\), meaning that the fraction an element has to constitute of \textit{S} to be in \textit{H} has an inverse relation to \textit{k}.
For each element in the data stream, the map is updated. If the key of the element already exists in the map, the value is incremented by 1. Else if the size of the map is less than \textit{k}, then the element is added to the map, with a value of 1. If the element does not exist in the map and the size of the map is \textit{k}, then the value of each key is decremented by 1 and keys with value zero is removed. This decrement procedure is repeated until the size of the map is less than \textit{k} such that the element can be added to the map.

\subsubsection{Frequency bounds}
An upper bound on \(c\) for an item in a stream can be determined from the resulting count of the item \(c_m\). It is obvious that the sum of counters, \(C\) in the resulting map can not be negative and the number of times it is increased is equal to \(m\). Each time it is decreased, \(C\) is reduced by \(k\) and the number of times this happens is denoted \(d\).\cite{accuracy}
\[C=m-dk\geq 0\]
Since the expression must be non-negative, the following is deducted.
\[d\leq \frac{m}{k}\]
As \(c_m\) has been reduced at most \(d\) times, the following upper bound on the value of \(c\) is deducted.
\[c_m \geq c-d \geq c-\frac{m}{k}\]
Hence the upper bound on the actual value of occurences \(c\).
\[c \leq c_m+\frac{m}{k}\]
A lower bound is obviously.
\[c \geq c_m\]

\subsubsection{Space consumption}
The auxiliary space used by Misra Gries is \(O\left(\frac{1}{\alpha}\right)\), but as \(\alpha\) is a constant in many applications, this is regarded as constant space consumption.\marginpar{Hvornår er alpha ikke en konstant}
